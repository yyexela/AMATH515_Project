{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Project Description\n",
    "\n",
    "Modern deep learning hinges on variants of stochastic gradient descent. Give an overview of some of the algorithms (ADAM,\n",
    "RMSProp, Adagrad,momentum, etc.) and stepsizing schemes (learning rate decay, cosine annealing, superconvergence, hyper-\n",
    "gradient learning rate adaptation, etc.) and perform an empirical comparison of their performance on some test problems of\n",
    "your choice. Obviously, no comparison here can be anywhere close to exhaustive due to the overabundance of deep learning op-\n",
    "timization papers, so just go through a handful of ideas that you find interesting. If you want to do this with realistic (somewhat\n",
    "large) networks, you’ll probably need access to some GPUs. This will take a bit more work (but should still be doable!) to do\n",
    "as a project if you don’t have any previous experience in deep learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Notes:\n",
    "- Test problems:\n",
    "  - [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "    - image classification\n",
    "  - [minGPT](https://github.com/karpathy/minGPT?tab=readme-ov-file)\n",
    "    - Text Generation (but trained by filling in missing words)\n",
    "  - [California Housing](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)\n",
    "    - Regression (predict housing cost)\n",
    "  - Simple 1D optimization to visualize things\n",
    "    - Rosenbrock function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Text Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available? True\n"
     ]
    }
   ],
   "source": [
    "# Reload external files when running cells\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add custom package to import path and import it\n",
    "file_dir = pathlib.Path().resolve()\n",
    "pkg_dir = os.path.join(file_dir, \"submodules\")\n",
    "sys.path.insert(0, pkg_dir)\n",
    "from amath515_pkg.src import *\n",
    "\n",
    "# Load minGTP\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import CharDataset, CfgNode as CN\n",
    "\n",
    "# Load config file\n",
    "pkg_config = helpers.get_config()\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(pkg_config['mingpt_np_seed'])\n",
    "torch.manual_seed(pkg_config['mingpt_torch_seed'])\n",
    "\n",
    "# Print matplotlibe plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sure Torch is installed and see if a GPU is available\n",
    "print(\"GPU Available?\",torch.cuda.is_available())\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tiny Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mingpt_config = helpers.get_minGPT_config()\n",
    "with open(os.path.join(file_dir, 'Datasets', 'tiny-shakespeare.txt'), 'r') as file:\n",
    "    tiny_shakespeare = file.read()\n",
    "train_dataset = CharDataset(mingpt_config.data, tiny_shakespeare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.71M\n",
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# construct the model\n",
    "mingpt_config.model.vocab_size = train_dataset.get_vocab_size()\n",
    "mingpt_config.model.block_size = train_dataset.get_block_size()\n",
    "model = GPT(mingpt_config.model)\n",
    "\n",
    "# prepare for training otherwise\n",
    "# construct the trainer object\n",
    "mingpt_config.trainer.max_iters = pkg_config['mingpt_iters']\n",
    "mingpt_config.trainer.optimizer_str = pkg_config['mingpt_SGD_method']\n",
    "mingpt_config.trainer.scheduler_str = pkg_config['mingpt_Scheduler']\n",
    "trainer = Trainer(mingpt_config.trainer, model, train_dataset)\n",
    "\n",
    "# construct callback function\n",
    "callback = lambda x: training.min_gpt_batch_end_callback(model, mingpt_config, train_dataset, x)\n",
    "trainer.set_callback('on_batch_end', callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### GPT Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, lr 0.000500\n",
      "---------------------------------\n",
      "loss: 3.069319 [iter 10 of 500]\n",
      "loss: 2.817652 [iter 20 of 500]\n",
      "loss: 2.711862 [iter 30 of 500]\n",
      "loss: 2.667384 [iter 40 of 500]\n",
      "loss: 2.665904 [iter 50 of 500]\n",
      "loss: 2.624243 [iter 60 of 500]\n",
      "loss: 2.635427 [iter 70 of 500]\n",
      "loss: 2.648400 [iter 80 of 500]\n",
      "loss: 2.633587 [iter 90 of 500]\n",
      "\n",
      "Iteration 100, lr 0.000452\n",
      "---------------------------------\n",
      "loss: 2.616637 [iter 110 of 500]\n",
      "loss: 2.612584 [iter 120 of 500]\n",
      "loss: 2.611892 [iter 130 of 500]\n",
      "loss: 2.590376 [iter 140 of 500]\n",
      "loss: 2.600641 [iter 150 of 500]\n",
      "loss: 2.610029 [iter 160 of 500]\n",
      "loss: 2.602420 [iter 170 of 500]\n",
      "loss: 2.605690 [iter 180 of 500]\n",
      "loss: 2.576747 [iter 190 of 500]\n",
      "\n",
      "Iteration 200, lr 0.000327\n",
      "---------------------------------\n",
      "loss: 2.597587 [iter 210 of 500]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run the optimization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/School/Classes/AMATH 515/Project/submodules/minGPT/mingpt/trainer.py:108\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mon_batch_end\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    110\u001b[0m tnow \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/School/Classes/AMATH 515/Project/submodules/minGPT/mingpt/trainer.py:60\u001b[0m, in \u001b[0;36mTrainer.trigger_callbacks\u001b[0;34m(self, onevent)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrigger_callbacks\u001b[39m(\u001b[38;5;28mself\u001b[39m, onevent: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mget(onevent, []):\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(mingpt_config\u001b[38;5;241m.\u001b[39mtrainer, model, train_dataset)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# construct callback function\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_gpt_batch_end_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmingpt_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m trainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, callback)\n",
      "File \u001b[0;32m~/School/Classes/AMATH 515/Project/submodules/amath515_pkg/src/training.py:139\u001b[0m, in \u001b[0;36mmin_gpt_batch_end_callback\u001b[0;34m(model, mingpt_config, train_dataset, trainer)\u001b[0m\n\u001b[1;32m    135\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39miter_num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m#print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>7f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39miter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmingpt_config\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39miter_num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m99\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/research/lib/python3.11/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# run the optimization\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.224329948425293, 3.7430996894836426, 3.5658071041107178, 3.5186662673950195, 3.4524800777435303, 3.3786368370056152, 3.321120500564575, 3.2594423294067383, 3.2242822647094727, 3.1626830101013184, 3.1144871711730957, 3.060476541519165, 3.0568454265594482, 3.0108633041381836, 2.9555728435516357, 2.9399607181549072, 2.918572187423706, 2.9067862033843994, 2.877592086791992, 2.8572659492492676, 2.8313610553741455, 2.809608221054077, 2.792649745941162, 2.7714765071868896, 2.784308671951294, 2.7812156677246094, 2.7605950832366943, 2.7403345108032227, 2.7056925296783447, 2.715184450149536, 2.713942289352417, 2.679215669631958, 2.7113289833068848, 2.7063956260681152, 2.6659536361694336, 2.7189600467681885, 2.7019662857055664, 2.6874468326568604, 2.69785213470459, 2.683434009552002, 2.6667754650115967, 2.669659376144409, 2.6945040225982666, 2.663917064666748, 2.6644721031188965, 2.663381338119507, 2.6398258209228516, 2.6669678688049316, 2.658080816268921, 2.6347594261169434]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(trainer.losses)\n",
    "print(trainer.iter_num)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
